# Why platforms aren't taking down deceptive political videos

*Not every meme is an epistemological crisis*

By [[Casey Newton]{.ul}](https://www.theverge.com/authors/casey-newton)[\@CaseyNewton](https://www.twitter.com/CaseyNewton)  Feb 11, 2020, 6:00am EST

## This story is part of a group of stories called![The Interface](media/image1.png){width="1.127658573928259in" height="0.6219892825896763in"}

*The Interface is a daily column and newsletter about the intersection of social [[media]] and democracy. Subscribe [here](https://www.getrevue.co/profile/caseynewton/).*

Say you run a large social network in which your most zealous users frequently discuss their [[Politics]]. In 2020, one way they are going to do this is through the sharing of memes --- pithy, punchy photos and videos designed for maximum partisan impact. Some of these memes will draw on actual facts; others will simply be insults. The most troublesome memes to deal with will be the ones that draw on real life but manipulate it in some way. These manipulations can be an essential part of satire, parody, and criticism. They can also trick people into believing a hoax. It's up to you to draw a bright line. Where do you draw it?

The question of manipulated [[media]] has come up in a big way twice in the past week. The first came when Twitter said it would label some manipulated and synthetic images starting next month. Here's [Adi Robertson in *[The Verge]{.ul}*](https://www.theverge.com/2020/2/4/21122661/twitter-deepfake-manipulated-[[media]]-policy-rollout-date):

Twitter will ban faked pictures, video, and other [[media]] that are "deceptively shared" and pose a serious safety risk. [The company just announced](https://blog.twitter.com/en_us/topics/company/2020/new-approach-to-synthetic-and-manipulated-[[media]].html) a new policy on synthetic and manipulated [[media]] --- a category that encompasses sophisticated deepfake videos, but also low-tech deceptively edited content. In addition to banning egregious offenders, Twitter will label some tweets as "manipulated [[media]]" and link to a Twitter Moment that provides more context.

The second appearance of manipulated [[media]] in the headlines came after the State of the Union address, when President Trump shared on his Twitter account a video that purported to show House Speaker Nancy Pelosi tearing up his speech during a series of feel-good moments during the speech. Here's [Drew Harwell and Tony Romm in the *[Washington Post]{.ul}*](https://www.washingtonpost.com/technology/2020/02/07/trump-shared-an-edited-pelosi-video-social-[[media]]-highlighting-gray-area-debate-over-disinformation/):

The viral video shows President Trump delivering his State of the Union address, with a very notable alteration. As he commemorates "Young [[Women]] Receiving Scholarships" and "Child Healthcare Successes," the video repeatedly cuts away to House Speaker Nancy Pelosi ripping up her copy of the speech.

It didn't actually happen that way: Pelosi (D-Calif.) tore the pages only after Trump finished what she later called his "manifesto of mistruths." But Trump on Thursday shared it anyway, sending it to millions of users on [Facebook](https://www.facebook.com/DonaldTrump/videos/vb.153080620724/173825713895904/?type=2&theater) and [Twitter](https://twitter.com/realDonaldTrump/status/1225553117929988097) --- and sparking sharp criticism from Pelosi and her fellow Democrats, who labeled the video "[doctored](https://twitter.com/RepDonBeyer/status/1225816845598547971)" and "[fake](https://twitter.com/RepEscobar/status/1225820699916427265)," and demanded that the sites remove it. The companies refused.

This was, of course, the second time that a doctored video of Pelosi made national headlines, following the incident last May in which [a video of her appearing drunk went viral](https://www.theverge.com/2019/5/24/18637771/nancy-pelosi-[[congress]]-deepfake-video-facebook-twitter-youtube). (In reality, the video's creator had simply [slowed her speech to 75 percent](https://www.youtube.com/watch?v=sDOo5nDJwgA) of its original speed.)

Lying has [a long tradition in American politics](https://www.amazon.com/dp/B002YKOXIY/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1&tag=theverge02-20). So why have the Pelosi videos created a panic? One, they erode our shared sense of reality by throwing into question the legitimacy of video evidence, a technology that before now we have generally regarded as trustworthy. And two, they suggest that in the future we will be unable to reliably tell fact from fiction, particularly on matters of intense public debate. (I think there's also probably a third fear here: that huge numbers of people will be misled into voting for the "wrong" candidate because they fell for one or more hoaxes.)

Hence Pelosi's spokesman calling for the doctored State of the Union video to be removed from Facebook. Here's [Jeff Horwitz and Natalie Andrews in the *[Wall Street Journal]{.ul}*](https://www.wsj.com/articles/edited-video-of-pelosi-ripping-trumps-speech-renews-debate-over-manipulated-content-11581115051):

Disagreements over the video triggered a spat on Twitter on Friday between Drew Hammill, Mrs. Pelosi's deputy chief of staff, and Andy Stone, a longtime Facebook spokesman. Mr. Hammill urged Facebook and Twitter to take down the video because it was "deliberately designed to mislead and lie to the American people."

To that, Mr. Stone responded, "Sorry, are you suggesting the President didn't make those remarks and the Speaker didn't rip the speech?"

Eight minutes later, Mr. Hammill shot back: "what planet are you living on? this is deceptively altered. take it down."

This is a perfect American debate over platforms in 2020, because it involves two people talking past one another without acknowledging any of the relevant tradeoffs, on a platform that rewards them for it with digital hearts.

Still, in this case, I'm with Facebook and Twitter --- this video should not be removed from the [[internet]]. As Stone notes, Pelosi *did* rip up Trump's speech on camera --- and she did not appear to avoid tearing up the nice bits where Trump praised a soldier or handed out a scholarship. In fact, the whole point of tearing up the speech on camera was for the act to be widely viewed and discussed. It's odd to engineer a moment like this one, [[purpose]] built for social [[media]], and then try to get a meme of it taken down.

Pelosi's people argue that showing the clips out of order represents an unacceptable distortion. But the video clearly re-uses the clip of Pelosi tearing the speech multiple times, making the fact that it's a chop job [[self]]-evident. Viewed in that light, Hammill's complaint reads more like film criticism than a call for platform policy [[reform]].

The truth is that there's likely no way to draw a line requiring the Pelosi video to be taken down that would also permit the kind of political speech we see every day on television. Any criticism that doesn't reckon with that fact strikes me as fundamentally glib.

Of course, it's also the case that political discourse on television --- particularly cable television --- is often terrible. A platform can embody high ideals of free speech and still be a pretty terrible place to become informed. It would be good for the country if, on that metric at least, Facebook and Twitter aimed much higher.

# 

# 5 ways to spot disinformation on your social media feeds

## ***Disinformation is everywhere. Here\'s how to sort real news from fake news.***

By

[**Erin Calabrese** January 7, 2020, 4:08 AM](https://abcnews.go.com/author/erin_calabrese)There's a lot that\'s been written about [disinformation and misinformation](https://abcnews.go.com/US/google-twitter-facebook-protect-2020-election-age-information/story?id=66527631) recently \-- the dark new reality of our increasingly connected and technologically advanced world that makes trusting what you see harder than ever. They're both forms of actual \"fake news,\" a term that once meant fake stories but has been co-opted by some right-wing leaders and activists to describe [[media]] organizations that they don\'t like.

Much disinformation (intentionally misleading) and misinformation (unintentionally misleading) is spread via social [[media]], so how do you spot these fake stories when they appear in your [Facebook](https://abcnews.go.com/alerts/facebook) feed, [Twitter](https://abcnews.go.com/alerts/twitter) timeline or [YouTube](https://abcnews.go.com/alerts/you-tube) playlist?

Recently, there was a great example of a story involving Michael Bloomberg's presidential campaign, which spread like wildfire among journalists on Twitter. A mock video and a mock campaign logo surfaced online. Neither was meant to be taken seriously, but many journalists immediately shared them because they thought all their friends would find them funny. Before long the false narratives were seen by thousands.

[MORE: [[Britain|UK]] election suggests disinformation spread by politicians may be a bigger threat in 2020 than [[Russia|Russians]] or 'deepfakes'](https://abcnews.go.com/International/uk-election-suggests-disinformation-spread-politicians-bigger-threat/story?id=67784381)

The best piece of advice to follow is to pause before you retweet or share, particularly if you have an emotional reaction and immediately think, "Oh I must share this.\" If you'd like to take a deeper dive into best verification practices, [First Draft News,](https://firstdraftnews.org/) a non-profit that helps journalists and others navigate the increasingly complicated digital sphere, has an [hour-long class](https://firstdraftnews.org/en/education/course/verification-quick-start/1/) to help you become a debugging pro. Below, we have a quick guide for determining whether or not you are looking at a piece of mis/disinformation.

Remember that the creators of disinformation purposely make content that is designed to trigger an emotional response, so if you find yourself having those reactions, please pause and consider the following questions.

>  Is this the original account, article, or piece of content?
>
>  Who shared this or created it?
>
>  When was this created?
>
>  What account is sharing this? When was the account created? Do they share things from all over the world at all times during the day and night? Could this be a bot?
>
>  Why was this shared?

If you use these questions and do some simple digging before sharing, you too can help prevent disinformation fires on social [[media]], here's how:

>  **[[Search]] online for the information or claim**. Sometimes, you'll be able to find fact-checkers online who have worked to debunk them. If the claim hasn't been reported widely by the press, there's a good chance this is because journalists couldn't confirm it.
>
>  **Look at who posted this content.** Inspect the poster\'s profile, how long their account has been active, and post history to see if they demonstrate bot-like behavior. For example if an account posts at all hours of the day, from different parts of the world, and includes highly polarizing political content and content retweeted from other accounts, those posts were likely made by a machine.
>
>  **Check the profile picture of the account**. Do a reverse image [[SEARCH]] of the photo. If it's a stock image or an image of a celebrity, then that's a less reliable source because it's anonymous..
>
>  **[[Search]] for other social [[media]] accounts for this person**. See what you can find out about that person, do they have political or religious affiliations that might give them a reason for spreading a particular point of view?
>
>  **Inspect the content the account posted**. Does it look too good to be true? If it does then it usually isn't real. Try a reverse image [[SEARCH]]. Using a tool like RevEye you can [[SEARCH]] for any previous instances of any image that appears online. Much disinformation uses old images out of context to push a narrative. Using reverse image [[SEARCH]] you can find if the image is from a different story. If you know the location of the image or video use 'Street View' mapping services (Google, Bing and others provide the service) to see if what you're looking at matches what appears on the map. You can also reverse image [[SEARCH]] the profile picture to see if it or similar photos are being used on other accounts, a common practice used to create so-called \"sockpuppet\" accounts, fake personas created online that allow people to act as trolls while protecting their [[self]].

There are many more sophisticated fact-checking tools that are available online for free. Bellingcat, a non-profit that carries out online visual investigations outlines many of them [here](https://docs.google.com/document/d/1BfLPJpRtyq4RFtHJoNpvWQjmGnyVkfE2HYoICKOGguA/edit)

However, the truth is that the vast majority of disinformation can be dismissed without using any of this technology. In many cases, by just asking the question, "Is this real?" and taking a couple of minutes to investigate, you will be able to verify or debunk the story. The problem is that in a social [[media]] age, many of us instinctively hit that share button, before we even think to ask that question.

We saw how disinformation was used in the 2016 election, and more recently [in the U.K. election](https://abcnews.go.com/International/uk-election-suggests-disinformation-spread-politicians-bigger-threat/story?id=67784381), so it's likely to be used even more extensively in 2020.

The social [[media]] platforms have taken steps to stem the flow of disinformation but ultimately the only way to stop it spreading is for consumers to stop sharing it.

So maybe before you hit that share button, next time just stop and think, is this real?

**The Billion-Dollar Disinformation Campaign to Reelect the President**

How new technologies and techniques pioneered by dictators will shape the 2020 election

**[MARCH 2020 ISSUE](https://www.theatlantic.com/magazine/toc/2020/03/) \--** <https://www.theatlantic.com/magazine/archive/2020/03/the-2020-disinformation-war/605530/> [**POLITICS**](https://www.theatlantic.com/politics/)

Bottom of Form

*Updated at 2:30 p.m. ET on February 10, 2020.*

O[ne day last fall]{.smallcaps}, I sat down to create a new Facebook account. I picked a forgettable name, snapped a profile pic with my face obscured, and clicked "Like" on the official pages of Donald Trump and his reelection campaign. Facebook's algorithm prodded me to follow Ann Coulter, Fox [[Business]], and a variety of fan pages with names like "In Trump We Trust." I complied. I also gave my cellphone number to the Trump campaign, and joined a handful of private Facebook groups for MAGA diehards, one of which required an application that seemed designed to screen out interlopers.

*To hear more feature stories, [[get the Audm iPhone app.]{.ul}](https://www.audm.com/?utm_source=soundcloud&utm_medium=embed&utm_campaign=atlantic&utm_content=2020_disinformation_war)*

The president's reelection campaign was then in the midst of a multimillion-dollar ad blitz aimed at shaping [[America|Americans]]' understanding of the recently launched impeachment proceedings. Thousands of micro-targeted ads had flooded the [[internet]], portraying Trump as a heroic reformer cracking down on foreign corruption while Democrats plotted a coup. That this narrative bore little resemblance to reality seemed only to accelerate its spread. Right-wing websites amplified every claim. Pro-Trump forums teemed with conspiracy theories. An alternate information ecosystem was taking shape around the biggest news story in the country, and I wanted to see it from the inside.

The story that unfurled in my Facebook feed over the next several weeks was, at times, disorienting. There were days when I would watch, live on TV, an impeachment hearing filled with damning testimony about the president's conduct, only to look at my phone later and find a slickly edited video---served up by the Trump campaign---that used out-of-context clips to recast the same testimony as an exoneration. *Wait*, I caught myself wondering more than once, *is *that* what happened today?*

As I swiped at my phone, a stream of pro-Trump [[propaganda]] filled the screen: "That's right, the whistleblower's own lawyer said, 'The coup has started ...' " *Swipe*. "Democrats are doing Putin's bidding ..." *Swipe*. "The only message these radical socialists and extremists will understand is a crushing ..." *Swipe*. "Only one man can stop this chaos ..." *Swipe*,* swipe*,* swipe*.

I was surprised by the effect it had on me. I'd assumed that my skepticism and [[media]] literacy would inoculate me against such distortions. But I soon found myself reflexively questioning *every* headline. It wasn't that I believed Trump and his boosters were telling the truth. It was that, in this state of heightened suspicion, truth itself---about Ukraine, impeachment, or anything else---felt more and more difficult to locate. With each swipe, the notion of observable reality drifted further out of reach.

What I was seeing was a strategy that has been deployed by illiberal political leaders around the world. Rather than shutting down dissenting voices, these leaders have learned to harness the democratizing power of social [[media]] for their own purposes---jamming the signals, sowing confusion. They no longer need to silence the dissident shouting in the streets; they can use a megaphone to drown him out. Scholars have a name for this: censorship through noise.

After the 2016 election, much was made of the threats posed to American democracy by foreign disinformation. Stories of Russian troll farms and Macedonian fake-news mills loomed in the national imagination. But while these shadowy outside forces preoccupied politicians and journalists, Trump and his domestic allies were beginning to adopt the same tactics of information warfare that have kept the world's demagogues and strongmen in power.

[*[Read: What, exactly, were [[Russia|Russians]] trying to do with those Facebook ads?]{.ul}*](https://www.theatlantic.com/technology/archive/2017/09/the-branching-possibilities-of-the-facebook-russian-ad-buy/541002/)

Every presidential campaign sees its share of spin and misdirection, but this year's contest promises to be different. In conversations with political strategists and other experts, a dystopian picture of the general election comes into view---one shaped by coordinated bot attacks, Potemkin local-news sites, micro-targeted fearmongering, and anonymous mass texting. Both parties will have these tools at their disposal. But in the hands of a president who lies constantly, who traffics in conspiracy theories, and who readily manipulates the levers of [[government]] for his own gain, their potential to wreak havoc is enormous.

The Trump campaign is planning to spend more than \$1 billion, and it will be aided by a vast coalition of partisan [[media]], outside political groups, and enterprising freelance operatives. These pro-Trump forces are poised to wage what could be the most extensive disinformation campaign in U.S. history. Whether or not it succeeds in reelecting the president, the wreckage it leaves behind could be irreparable.

**DISINFORMATION ARCHITECTURE**

In his book [*[This Is Not Propaganda]{.ul}*](https://www.publicaffairsbooks.com/titles/peter-pomerantsev/this-is-not-propaganda/9781541762138/), Peter Pomerantsev, a researcher at the London School of [[economy|Economics]], writes about a young Filipino political consultant he calls "P." In college, P had studied the "Little Albert experiment," in which scientists conditioned a young child to fear furry animals by exposing him to loud noises every time he encountered a white lab rat. The experiment gave P an idea. He created a series of Facebook groups for Filipinos to discuss what was going on in their communities. Once the groups got big enough---about 100,000 members---he began posting local crime stories, and instructed his employees to leave comments falsely tying the grisly headlines to drug cartels. The pages lit up with frightened chatter. Rumors swirled; conspiracy theories metastasized. To many, all crimes became drug crimes.

Unbeknownst to their members, the Facebook groups were designed to boost Rodrigo Duterte, then a long-shot presidential candidate running on a pledge to brutally crack down on drug criminals. ([[Duterte once boasted]{.ul}](https://www.nytimes.com/2016/12/14/world/asia/rodrigo-duterte-philippines-killings.html) that, as mayor of Davao City, he rode through the streets on his motorcycle and personally executed drug dealers.) P's experiment was one plank in a larger "disinformation architecture"---which also included social-[[media]] influencers paid to mock opposing candidates, and mercenary trolls working out of former call centers---that experts say aided Duterte's rise to power. Since assuming office in 2016, Duterte has reportedly ramped up these efforts while presiding over thousands of extrajudicial killings.

The [[campaign in the Philippines]{.ul}](http://newtontechfordev.com/wp-content/uploads/2018/02/Architects-of-Networked-Disinformation-Executive-Summary-Final.pdf) was emblematic of an emerging [[propaganda]] playbook, one that uses new tools for the age-old ends of autocracy. The [[Kremlin has long been an innovator in this area]{.ul}](https://time.com/5722805/rethink-information-[[war]]-russia/). (A 2011 manual for Russian civil servants favorably compared their methods of disinformation to "an invisible radiation" that takes effect while "the population doesn't even feel it is being acted upon.") But with the technological advances of the past decade, and the global proliferation of smartphones, governments around the world have found success deploying Kremlin-honed techniques against their own people.

[*[Read: Peter Pomerantsev on [[Russia]] and the menace of unreality]{.ul}*](https://www.theatlantic.com/international/archive/2014/09/russia-putin-revolutionizing-information-warfare/379880/)

In [[America|the United States]], we tend to view such tools of oppression as the faraway problems of more fragile democracies. But the people working to reelect Trump understand the power of these tactics. They may use gentler terminology---*muddy the waters*;* alternative facts*---but they're building a machine designed to exploit their own sprawling disinformation architecture.

Central to that effort is the campaign's use of micro-targeting---the process of slicing up the electorate into distinct niches and then appealing to them with precisely tailored digital messages. The advantages of this approach are obvious: An ad that calls for defunding Planned Parenthood might get a mixed response from a large national audience, but serve it directly via Facebook to 800 Roman Catholic [[women]] in Dubuque, Iowa, and its reception will be much more positive. If candidates once had to shout their campaign promises from a soapbox, micro-targeting allows them to sidle up to millions of voters and whisper personalized messages in their ear.

Parscale didn't invent this practice---Barack Obama's campaign famously used it in 2012, and Clinton's followed suit. But Trump's effort in 2016 was unprecedented, in both its scale and its brazenness. In the final days of the 2016 race, for example, Trump's team tried to suppress turnout among black voters in Florida by slipping ads into their News Feeds that read, "Hillary Thinks African-[[America|Americans]] Are Super Predators." An unnamed campaign official boasted to *Bloomberg Businessweek *that it was one of "[[three major voter suppression operations underway]{.ul}](https://www.bloomberg.com/news/articles/2016-10-27/inside-the-trump-bunker-with-12-days-to-go)." (The other two targeted young [[women]] and white liberals.)

The weaponization of micro-targeting was pioneered in large part by the data scientists at Cambridge Analytica. The firm began as part of a nonpartisan military contractor that used digital psyops to target terrorist groups and drug cartels. In Pakistan, it worked to thwart jihadist recruitment efforts; in South [[America]], it circulated disinformation to turn drug dealers against their bosses.

The emphasis shifted once the conservative billionaire Robert Mercer became a major investor and installed Steve Bannon as his point man. Using a massive trove of data it had gathered from Facebook and other sources---without users' consent---Cambridge Analytica worked [[to develop detailed "psychographic profiles"]{.ul}](https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html) for every voter in the U.S., and began experimenting with ways to stoke paranoia and bigotry by exploiting certain personality traits. In one exercise, the firm asked white men whether they would approve of their daughter marrying a Mexican immigrant; those who said yes were asked a follow-up question designed to provoke irritation at the constraints of political correctness: "Did you feel like you had to say that?"

Christopher Wylie, who was the director of research at Cambridge Analytica and later testified about the company to [[Congress]], told me that "with the right kind of nudges," people who exhibited certain psychological characteristics could be pushed into ever more extreme beliefs and conspiratorial thinking. "Rather than using data to interfere with the process of radicalization, Steve Bannon was able to invert that," Wylie said. "We were essentially seeding an insurgency in [[America|the United States]]."

Cambridge Analytica was dissolved in 2018, shortly after its CEO was caught on tape bragging about using bribery and sexual "honey traps" on behalf of clients. (The firm denied that it actually used such tactics.) Since then, some political scientists have questioned how much effect its "psychographic" targeting really had. But Wylie---who spoke with me from London, where he now works for H&M, as a fashion-trend forecaster---said the firm's work in 2016 was a modest test run compared with what could come.

"What happens if North Korea or Iran picks up where Cambridge Analytica left off?" he said, noting that plenty of foreign actors will be looking for ways to interfere in this year's election. "There are countless hostile states that have more than enough capacity to quickly replicate what we were able to do ... and make it much more sophisticated." These efforts may not come only from abroad: A group of former Cambridge Analytica employees have formed a new firm that, [[according to the Associated Press]{.ul}](https://apnews.com/96928216bdc341ada659447973a688e4), is working with the Trump campaign. (The firm has denied this, and a campaign spokesperson declined to comment.)

After the Cambridge Analytica scandal broke, [[Facebook was excoriated for its mishandling of user data]{.ul}](https://www.nytimes.com/2018/03/17/us/politics/cambridge-analytica-trump-campaign.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news) and complicity in the viral spread of fake news. Mark Zuckerberg promised to do better, and rolled out a flurry of reforms. But then, last fall, he handed a major victory to lying politicians: Candidates, he said, would be allowed to continue running false ads on Facebook. (Commercial advertisers, by contrast, are subject to fact-checking.) In [[a speech at Georgetown University]{.ul}](https://www.facebook.com/zuck/videos/10109815371842941/?notif_id=1571332000346664&notif_t=live_video), the CEO argued that his company [[shouldn't be responsible for arbitrating political speech]{.ul}](https://www.nytimes.com/2019/10/17/business/zuckerberg-facebook-free-speech.html), and that because political ads already receive so much scrutiny, candidates who choose to lie will be held accountable by journalists and watchdogs.

Shady political actors are discovering how easy it is to wage an untraceable whisper campaign by text message.

To bolster his case, Zuckerberg pointed to the recently launched---and publicly accessible---[["library" where Facebook archives every political ad it publishes]{.ul}](https://www.facebook.com/ads/library/). The project has a certain [[democratic]] appeal: Why censor false or toxic content when a little sunlight can have the same effect? But spend some time scrolling through the archive of Trump reelection ads, and you quickly see the limits of this transparency.

[*[Read: The age of reverse censorship]{.ul}*](https://www.theatlantic.com/politics/archive/2018/06/is-the-first-amendment-obsolete/563762/)

The campaign doesn't run just one ad at a time on a given theme. It runs hundreds of iterations---adjusting the language, the music, even the colors of the "Donate" buttons. In the 10 weeks after the House of Representatives began its impeachment inquiry, the Trump campaign ran roughly 14,000 different ads containing the word *impeachment*. Sifting through all of them is virtually impossible.

Both parties will rely on micro-targeted ads this year, but the president is likely to have a distinct advantage. The Republican National Committee and the Trump campaign have reportedly compiled an average of [[3,000 data points on every voter in America]{.ul}](https://www.csmonitor.com/USA/Politics/2019/1217/Watch-golf-Own-guns-Trump-data-team-has-ads-just-for-you). They have spent years experimenting with ways to tweak their messages based not just on [[gender]] and geography, but on whether the recipient owns a gun or watches the Golf Channel.

While these ads can be used to try to win over undecided voters, they're most often deployed for fundraising and for firing up the faithful---and Trump's advisers believe this election will be decided by mobilization, not persuasion. To turn out the base, the campaign has signaled that it will return to familiar themes: the threat of "illegal aliens"---[[a term Parscale has reportedly encouraged Trump to use]{.ul}](https://www.washingtonpost.com/politics/how-brad-parscale-once-a-nobody-in-san-antonio-shaped-trumps-combative-[[politics]]-and-rose-to-his-inner-circle/2018/11/09/b4257d58-dbb7-11e8-b3f0-62607289efee_story.html)---and the corruption of the "swamp."

Beyond Facebook, the campaign is also investing in a texting platform that could allow it to send anonymous messages directly to millions of voters' phones without their permission. Until recently, people had to opt in before a campaign could include them in a mass text. But with new "peer to peer" texting apps---including one developed by Gary Coby, a senior Trump adviser---[[a single volunteer can send hundreds of messages an hour]{.ul}](https://www.vice.com/en_us/article/vbjjw9/text-campaigns-are-changing-american-[[politics]]-and-nobodys-ready), skirting federal regulations by clicking "Send" one message at a time. Notably, these messages aren't required to disclose who's behind them, thanks to a 2002 ruling by the Federal Election Commission that cited the limited number of characters available in a text.

Most experts assume that these regulations will be overhauled sometime after the 2020 election. For now, campaigns from both parties are hoovering up as many cellphone numbers as possible, and Parscale has said texting will be at the center of Trump's reelection strategy. The medium's ability to reach voters is unparalleled: While robocalls get sent to voicemail and email blasts get trapped in spam folders, peer-to-peer texting companies say that at least 90 percent of their messages are opened.

The Trump campaign's texts so far this cycle have focused on shouty fundraising pleas ("They have NOTHING! IMPEACHMENT IS OVER! Now let's CRUSH our End of Month Goal"). But the potential for misuse by outside groups is clear---and shady political actors are already discovering how easy it is to wage an untraceable whisper campaign by text.

In 2018, as early voting got under way in Tennessee's Republican gubernatorial primary, [[voters began receiving text messages]{.ul}](https://www.tennessean.com/story/news/politics/tn-elections/2018/07/13/tn-governors-race-tennesseans-receive-potentially-illegal-text-messages-attacking-randy-boyd-billlee/783785002/) attacking two of the candidates' conservative credentials. The texts---written in a conversational style, as if they'd been sent from a friend---were unsigned, and people who tried calling the numbers received a busy signal. The local press covered the smear campaign. Law enforcement was notified. But the source of the texts was never discovered.

**WAR ON THE PRESS**

One afternoon last March, I was on the phone with a [[Republican]] operative close to the Trump family when he casually mentioned that a reporter at *[[Business]] Insider *was about to have a very bad day. The journalist, John Haltiwanger, had tweeted something that annoyed Donald Trump Jr., prompting the coterie of friends and allies surrounding the president's son to drum up a hit piece. The story they had coming, the operative suggested to me, would demolish the reporter's credibility.

I wasn't sure what to make of this gloating---people in Trump's circle have a tendency toward bluster. But a few hours later, the operative sent me a link to a *Breitbart News* article documenting Haltiwanger's "history of intense Trump hatred." The story was based on a series of Instagram posts---all of them from before Haltiwanger started working at *[[Business]] Insider*---in which he made fun of the president and expressed solidarity with liberal protesters.

The next morning, Don Jr. [[tweeted the story]{.ul}](https://twitter.com/donaldjtrumpjr/status/1108705539951939584?lang=en) to his 3 million followers, denouncing Haltiwanger as a "raging lib." Other conservatives piled on, and the reporter was bombarded with abusive messages and calls for him to be fired. His employer issued a statement conceding that the Instagram posts were "not appropriate." Haltiwanger kept his job, but the experience, he told me later, "was bizarre and unsettling."

The *Breitbart* story was part of a coordinated effort by a coalition of Trump allies to air embarrassing information about reporters who produce critical coverage of the president. (*The New York Times* [[first reported on this project]{.ul}](https://www.nytimes.com/2019/08/25/us/politics/trump-allies-news-[[media]].html) last summer; since then, it's been described to me in greater detail.) According to people with knowledge of the effort, pro-Trump operatives have scraped social-[[media]] accounts belonging to hundreds of political journalists and compiled years' worth of posts into a dossier.

Often when a particular news story is deemed especially unfair---or politically damaging---to the president, Don Jr. will flag it in a text thread that he uses for this [[purpose]]. (Among those who text regularly with the president's eldest son, someone close to him told me, are the conservative activist Charlie Kirk; two GOP strategists, Sergio Gor and Arthur Schwartz; Matthew Boyle, a *Breitbart* editor; and U.S. Ambassador Richard Grenell.) Once a story has been marked for attack, someone searches the dossier for material on the journalists involved. If something useful turns up---a problematic old joke; evidence of liberal political views---Boyle turns it into a *Breitbart* headline, which White House officials and campaign surrogates can then share on social [[media]]. (The White House has denied any involvement in this effort.)

Descriptions of the dossier vary. One source I spoke with said that a programmer in India had been paid to organize it into a searchable database, making posts that contain offensive keywords easier to find. Another told me the dossier had expanded to at least 2,000 people, including not just journalists but high-profile academics, politicians, celebrities, and other potential Trump foes. Some of this, of course, may be hyperbolic boasting---but the effort has yielded fruit.

Parscale has said the campaign intends to train "swarms of surrogates" to undermine coverage from local TV stations and newspapers.

In the past year, the operatives involved have gone after journalists at CNN, *The* *Washington Post*, and *The* *New York Times*. They exposed one reporter for [[using the word *fag* in college]{.ul}](https://www.washingtonpost.com/blogs/erik-wemple/wp/2018/10/08/cnn-reporter-kaitlan-collins-under-fire-for-seven-year-old-homophobic-tweets/), and another for [[posting anti-Semitic and racist jokes a decade ago]{.ul}](https://www.washingtonexaminer.com/news/crappy-jew-year-new-york-times-editor-apologizes-for-offensive-tweets). These may not have been career-ending revelations, but people close to the project said they're planning to unleash much more opposition research as the campaign intensifies. "This is innovative shit," said Mike Cernovich, a right-wing activist with a history of trolling. "They're appropriating call-out culture."

What's notable about this effort is not that it aims to expose [[media]] bias. Conservatives have been complaining---with some merit---about a liberal slant in the press for decades. But in the Trump era, an important shift has taken place. Instead of trying to [[reform]] the press, or critique its coverage, today's most influential conservatives [[want to destroy the mainstream [[media]] altogether]{.ul}](https://www.breitbart.com/the-media/2017/07/19/breitbarts-boyle-goal-elimination-entire-mainstream-media/). "Journalistic integrity is dead," Boyle declared in a 2017 speech at the Heritage Foundation. "There is no such thing anymore. So everything is about weaponization of information."

It's a lesson drawn from demagogues around the world: When the press as an institution is weakened, fact-based journalism becomes just one more drop in the daily deluge of content---no more or less credible than partisan [[propaganda]]. Relativism is the real goal of Trump's assault on the press, and the more "enemies of the people" his allies can take out along the way, the better. "[[A culture [[war]] is a war]{.ul}](https://www.nytimes.com/2019/08/25/us/politics/trump-allies-news-[[media]].html)," Steve Bannon told the *Times* last year. "There are casualties in [[war]]."

This attitude has permeated the president's base. At rallies, people wear T-shirts that read [rope. tree. journalist. some assembly required.]{.smallcaps} A *CBS News*/YouGov poll has found that [[just 11 percent of strong Trump supporters trust the mainstream media]{.ul}](https://www.esquire.com/news-politics/a22600827/donald-trump-supporters-believe-the-media/)---while 91 percent turn to the president for "accurate information." This dynamic makes it all but impossible for the press to hold the president accountable, something Trump himself seems to understand. "Remember," he told a crowd in 2018, "[[what you're seeing and what you're reading is not what's happening]{.ul}](https://www.bbc.com/news/av/world-us-canada-44959340/donald-trump-what-you-re-seeing-and-what-you-re-reading-is-not-what-s-happening)."

Bryan Lanza, who worked for the Trump campaign in 2016 and remains a White House surrogate, told me flatly that he sees no possibility of [[America|Americans]] establishing a common set of facts from which to conduct the big debates of this year's election. Nor is that his goal. "It's our job to sell our narrative louder than the [[media]]," Lanza said. "They're clearly advocating for a liberal-socialist position, and we're never going to be in concert. So the [[war]] continues."

[*[From December 2019: The dark psychology of social networks]{.ul}*](https://www.theatlantic.com/magazine/archive/2019/12/social-[[media]]-democracy/600763/)

Parscale has indicated that he plans to open up a new front in this [[war]]: local news. Last year, he said the campaign [[intends to train "swarms of surrogates"]{.ul}](https://www.palmbeachpost.com/news/20190531/brad-parscale-genius-who-won-trumps-campaign-and-how-hell-get-him-reelected) to undermine negative coverage from local TV stations and newspapers. Polls have long found that [[America|Americans]] across the political spectrum trust local news more than national [[media]]. If the campaign has its way, that trust will be eroded by November. "We can actually build up and fight with the local newspapers," Parscale told donors, according to a recording provided by *The* *Palm Beach Post*. "So we're not just fighting on Fox News, CNN, and MSNBC with the same 700,000 people watching every day."

Running parallel to this effort, some conservatives have been experimenting with [[a scheme to exploit the credibility of local journalism]{.ul}](https://www.nytimes.com/2019/10/31/upshot/fake-local-news.html). Over the past few years, hundreds of websites with innocuous-sounding names like *the Arizona Monitor* and *The Kalamazoo Times *have begun popping up. [[At first glance, they look like regular publications, complete with community notices and coverage of schools]{.ul}](https://www.cjr.org/tow_center_reports/hundreds-of-pink-slime-local-news-outlets-are-distributing-algorithmic-stories-conservative-talking-points.php). But look closer and you'll find that there are often no mastheads, few if any bylines, and no addresses for local offices. Many of them are organs of [[Republican]] lobbying groups; others belong to a mysterious company called Locality Labs, which is run by a conservative activist in Illinois. Readers are given no indication that these sites have political agendas---which is precisely what makes them valuable.

According to one longtime strategist, candidates looking to plant a negative story about an opponent can pay to have their desired headlines posted on some of these Potemkin news sites. By working through a third-party consulting firm---instead of paying the sites directly---candidates are able to obscure their involvement in the scheme when they file expenditures to the Federal Election Commission. Even if the stories don't fool savvy readers, the headlines are convincing enough to be flashed across the screen in a campaign commercial or slipped into fundraising emails.

**DIGITAL DIRTY TRICKS**

Shortly after polls closed in Kentucky's gubernatorial election last November, an anonymous Twitter user named \@Overlordkraken1 announced to his 19 followers that he had "just shredded a box of [[Republican]] mail in ballots" in Louisville.

There was little reason to take this claim at face value, and plenty of reason to doubt it (beginning with the fact that he'd misspelled *Louisville*). But the race was tight, and as incumbent Governor Matt Bevin began to fall behind in the vote total, [[an army of Twitter bots began spreading the election-rigging claim]{.ul}](https://www.nytimes.com/2019/11/10/us/politics/kentucky-election-disinformation-twitter.html).

The original post was removed by Twitter, but by then thousands of automated accounts were circulating screenshots of it with the hashtag \#StoptheSteal. Popular right-wing internet personalities jumped on the narrative, and soon the Bevin campaign was making noise about unspecified voting "irregularities." When the race was called for his opponent, the governor refused to concede, and asked for a statewide review of the vote. (No evidence of ballot-shredding was found, and he finally [[admitted defeat nine days later]{.ul}](https://www.cnn.com/2019/11/14/politics/kentucky-governor-recanvas-begins/index.html).)

The Election Night disinformation blitz had all the markings of a foreign influence operation. In 2016, Russian trolls had worked in similar ways to contaminate U.S. political discourse---[[posing as Black Lives Matter activists]{.ul}](https://www.usatoday.com/story/news/politics/2018/12/17/russia-social-media-senate-report/2334382002/) in an attempt to inflame racial divisions, and fanning pro-Trump conspiracy theories. (They even used Facebook to organize rallies, including one for Muslim supporters of Clinton in Washington, D.C., where they [[got someone to hold up a sign]{.ul}](https://www.businessinsider.com/russians-organized-pro-anti-trump-rallies-to-sow-discord-2018-2#july-9-2016-washington-dc-2) attributing a fictional quote to the candidate: "I think Sharia law will be a powerful new direction of freedom.")

But when Twitter employees later reviewed the activity surrounding Kentucky's election, they concluded that the [[bots were largely based in America]{.ul}](https://www.nytimes.com/2019/11/10/us/politics/kentucky-election-disinformation-twitter.html)---a sign that political operatives here were learning to mimic Russian trolling tactics.

Of course, dirty tricks aren't new to American [[Politics]]. From Lee Atwater and Roger Stone to the crooked machine Democrats of Chicago, the country has a long history of underhanded operatives smearing opponents and meddling in elections. And, in fact, Samuel Woolley, [[a scholar who studies digital propaganda]{.ul}](https://www.publicaffairsbooks.com/titles/samuel-woolley/the-reality-game/9781541768253/), told me that the first documented deployment of politicized Twitter bots was in the U.S. In 2010, an Iowa-based conservative group set up a small network of automated accounts with names like \@BrianD82 to promote the idea that Martha Coakley, a [[Democrat]] running for Senate in Massachusetts, was anti-Catholic.

Since then, the tactics of Twitter warfare have grown more sophisticated, as regimes around the world experiment with new ways to deploy their cybermilitias. In [[Mexico]], supporters of then-President Enrique Peña Nieto created "sock puppet" accounts to pose as protesters and sabotage the opposition movement. In Azerbaijan, a pro-[[government]] youth group waged coordinated harassment campaigns against journalists, flooding their Twitter feeds with graphic threats and insults. When these techniques prove successful, Woolley told me, [[America|Americans]] improve upon them. "It's almost as if there's a Columbian exchange between developing-world authoritarian regimes and the West," he said.

Parscale has denied that the campaign uses bots, [[saying in a *60 Minutes *interview]{.ul}](https://www.cbs.com/shows/60_minutes/video/elHhrLFmOS2ZYFqRG68KQPAu0_aUKPKC/who-is-brad-parscale-/), "I don't think \[they\] work." He may be right---it's unlikely that these nebulous networks of trolls and bots could swing a national election. But they do have their uses. They can simulate false consensus, derail sincere debate, and hound people out of the public square.

According to one study, [[bots accounted for roughly 20 percent]{.ul}](https://firstmonday.org/article/view/7090/5653) of all the tweets posted about the 2016 election during one five-week period that year. And Twitter is already infested with bots that seem designed to boost Trump's reelection prospects. Regardless of where they're coming from, they have tremendous potential to divide, radicalize, and stoke hatred that lasts long after the votes are cast.

Rob Flaherty, who served as the digital director for Beto O'Rourke's presidential campaign, told me that Twitter in 2020 is a "hall of mirrors." He said one mysterious account started [[a viral rumor]{.ul}](https://www.cbsnews.com/news/orourke-campaign-responds-to-bot-promoted-conspiracy-theory-that-odessa-shooter-was-a-supporter/) that the gunman who killed seven people in Odessa, Texas, last summer had a [beto]{.smallcaps} bumper sticker on his car. Another masqueraded as an O'Rourke supporter and hurled racist invective at a journalist. Some of these tactics echoed 2016, when [[Russian agitators posed as [[Bernie Sanders]] supporters]{.ul}](https://www.salon.com/2019/04/12/new-data-suggests-[[Russia|russians]]-targeted-bernie-sanders-voters-to-help-elect-trump/) and stirred up anger toward Hillary Clinton.

Flaherty said he didn't know who was behind the efforts targeting O'Rourke, and the candidate dropped out before they could make a real difference. "But you can't watch this landscape and not get the feeling that someone's fucking with something," he told me. Flaherty has since joined Joe Biden's campaign, which has had to contend with similar distortions: Last year, a website resembling an official Biden campaign page appeared on the internet. It emphasized elements of the candidate's legislative record likely to hurt him in the Democratic primary---opposition to same-sex marriage, support for the Iraq War---and featured video clips of his awkward encounters with women. The site quickly became one of the most-visited Biden-related sites on the web. [[It was designed by a Trump consultant]{.ul}](https://www.nytimes.com/2019/06/29/us/politics/fake-joe-biden-website.html).

**FIGHTING FIRE WITH FIRE**

As the president's reelection machine ramps up, [[Democratic]] strategists have found themselves debating an urgent question: Can they defeat the Trump coalition without adopting its tactics?

On one side of this argument is Dmitri Mehlhorn, a consultant notorious for his willingness to experiment with digital subterfuge. During Alabama's special election in 2017, Mehlhorn helped fund at least two "false flag" operations against the [[Republican]] Senate candidate, Roy Moore. For one scheme, [[faux Russian Twitter bots followed the candidate's account]{.ul}](https://www.nytimes.com/2018/12/19/us/alabama-senate-roy-jones-[[russia]].html) to make it look like the Kremlin was backing Moore. For another, a fake social-[[media]] campaign, dubbed "Dry Alabama," was designed to link Moore to fictional Baptist teetotalers trying to ban alcohol. (Mehlhorn has claimed that he unaware of the Russian bot effort** **and does not support the use of misinformation.)

When *The* *New York Times* [[uncovered the second plot, one of the activists involved, Matt Osborne, contended]{.ul}](https://www.nytimes.com/2019/01/07/us/politics/alabama-senate-facebook-roy-moore.html) that Democrats had no choice but to employ such unscrupulous techniques. "If you don't do it, you're fighting with one hand tied behind your back," Osborne said. "You have a moral imperative to do this---to do whatever it takes."

Others have argued that this is precisely the wrong moment for Democrats to start abandoning ideals of honesty and [[fairness]]. "It's just not in my values to go out there making shit up and tricking voters," Flaherty told me. "I know there's this whole fight-fire-with-fire contingent, but generally when you ask them what they mean, they're like, 'Lie!' " Some also note that the president has already handed them plenty of ammunition. "I don't think the [[Democratic]] campaign is going to need to make stuff up about Trump," Judd Legum, the author of a progressive newsletter about digital [[Politics]], told me. "They can stick to things that are true."

Eventually, the fear of covert [[propaganda]] inflicts as much damage as the [[propaganda]] itself.

One [[Democrat]] straddling these two camps is a young, tech-savvy strategist named Tara McGowan. Last fall, she and the former [[Obama]] adviser David Plouffe launched a political-action committee with a pledge to spend \$75 million attacking Trump online. At the time, the president's campaign was running more ads on Facebook and Google than the top four [[Democratic]] candidates combined. McGowan's plans to return fire included such ads, but she also had more creative---and controversial---measures in mind.

For example, [[she established a [[media]] organization with a staff of writers]{.ul}](https://www.bloomberg.com/news/features/2019-11-25/acronym-s-newsrooms-are-a-liberal-digital-spin-on-local-news) to produce left-leaning "hometown news" stories that can be micro-targeted to persuadable voters on Facebook without any indication that they're paid for by a political group. Though she insists that the reporting is strictly factual, some see the enterprise as a too-close-for-comfort co-opting of right-wing tactics.

When I spoke with McGowan, she was open about her willingness to push boundaries that might make some Democrats queasy. As far as she was concerned, the "super-predator" ads Trump ran to depress black turnout in 2016 were "fair game" because they had some basis in fact. (Clinton did use the term in 1996, to refer to gang members.) McGowan suggested that a similar approach could be taken with conservatives. [[She ruled out attempts to misinform Republicans about when and where to vote]{.ul}](http://nymag.com/intelligencer/2019/09/facebook-silicon-valley-[[democratic]]-party.html)---a tactic Mehlhorn reportedly considered, though he later said he was joking---but said she would pursue any strategy that was "in the bounds of the law."

"We are in a radically disruptive moment right now," McGowan told me. "We have a president that lies every day, unabashedly ... I think Trump is so desperate to win this election that he will do anything. There will be no bar too low for him."

This intraparty split was highlighted last year when state officials urged the [[Democratic]] National Committee to formally disavow the use of bots, troll farms, and "deepfakes" (digitally manipulated videos that can, with alarming precision, make a person appear to do or say anything). Supporters saw the proposed pledge as a way of contrasting their party's values with those of the GOP. But after months of lobbying, the committee refused to adopt the pledge.

[*[From May 2018: The era of fake video begins]{.ul}*](https://www.theatlantic.com/magazine/archive/2018/05/realitys-end/556877/)

Meanwhile, experts worried about domestic disinformation are looking to other countries for lessons. The most successful recent example may be Indonesia, which cracked down on the problem after a wave of viral lies and conspiracy theories pushed by hard-line Islamists led to the defeat of a popular [[Christianity|Christian]] Chinese candidate for governor in 2016. To prevent a similar disruption in last year's presidential election, a coalition of journalists from more than two dozen top Indonesian news outlets worked together to identify and debunk hoaxes before they gained traction online. But while that may sound like a promising model, it was paired with aggressive efforts by the state to monitor and arrest purveyors of fake news---an approach that would run afoul of the First Amendment if attempted in the U.S.

Richard Stengel, who served as the undersecretary of state for public diplomacy under President [[Obama]], spent almost three years trying to counter digital [[propaganda]] from the Islamic State and [[Russia]]. By the time he left office, he told me, he was convinced that disinformation would continue to thrive until big tech companies were forced to take responsibility for it. Stengel has proposed amending the 1996 Communications Decency Act, which shields online platforms from liability for messages posted by third parties. Companies such as Facebook and Twitter, he believes, should be required by law to police their platforms for disinformation and abusive trolling. "It's not going to solve the whole problem," he told me, "but it's going to help with volume."

There is one other case study to consider. During the Ukrainian [[revolution]] in 2014, pro-democracy activists found that they could defang much of the false information about their movement by repeatedly exposing its Russian origins. But this kind of transparency comes with a cost, Stengel observed. Over time, alertness to the prevalence of [[propaganda]] can curdle into paranoia. Russian operatives have been known to encourage such [[anxiety]] by spreading rumors that exaggerate their own influence. Eventually, the fear of covert [[propaganda]] inflicts as much damage as the [[propaganda]] itself.

Once you internalize the possibility that you're being manipulated by some hidden hand, nothing can be trusted. Every dissenting voice on Twitter becomes a Russian bot, every uncomfortable headline a false flag, every political development part of an ever-deepening conspiracy. By the time the information ecosystem collapses under the weight of all this cynicism, you're too vigilant to notice that the disinformationists have won.

...The political theorist Hannah Arendt once wrote that the most successful totalitarian leaders of the 20th century instilled in their followers "a mixture of gullibility and cynicism." When they were lied to, they chose to believe it. When a lie was debunked, they claimed they'd known all along---and would then "admire the leaders for their superior tactical cleverness." Over time, Arendt wrote, the onslaught of [[propaganda]] conditioned people to "believe everything and nothing, think that everything was possible and that nothing was true." ...Should this ethos prevail in 2020, the election's legacy will be clear---not a choice between parties or candidates or policy platforms, but a referendum on reality itself.

*This article appears in the March 2020 print edition with the headline "The 2020 Disinformation [[War]]."*

[**MCKAY COPPINS**](https://www.theatlantic.com/author/mckay-coppins/)* is a staff writer at *The Atlantic* and the author of *[[The Wilderness]{.ul}](http://www.amazon.com/exec/obidos/ISBN=0316327417/theatla05-20/)*, a book about the battle over the future of the [[Republican]] Party.*
